{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd \n",
    "import os \n",
    "import plotly.io as pio\n",
    "pio.kaleido.scope.mathjax = None \n",
    "from math import log\n",
    "import numpy as np \n",
    "import math "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>VLMC Analysis</b>\n",
    "This notebook is devoted to analysing VLMCs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amount of matching kmers when comparing two VLMCs\n",
    "Here we count which kmers are matched and not matched in two VLMCs that are compared by our distance calculation. By design the distance calculation selects the left VLMC to be the shorted of the two VLMCs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iloc_x = 1000\n",
    "rolling_x = 10000\n",
    "\n",
    "df_left = pd.read_csv(\"./tmp/distributions/left_distribution_turkey_to_human.txt\", sep=\",\", header=None)\n",
    "df_right = pd.read_csv(\"./tmp/distributions/right_distribution_turkey_to_human.txt\", sep=\",\", header=None)\n",
    "df_left.columns = ['match']\n",
    "df_left['not_match'] = df_left.match != 1\n",
    "df_left['in_a_row'] = df_left['not_match'].cumsum()-df_left['not_match'].cumsum().where(~df_left['not_match']).ffill().fillna(0).astype(int)\n",
    "\n",
    "df_right.columns = ['match']\n",
    "df_right['not_match'] = df_right.match != 1\n",
    "df_right['in_a_row'] = df_right['not_match'].cumsum()-df_right['not_match'].cumsum().where(~df_right['not_match']).ffill().fillna(0).astype(int)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_left.iloc[::iloc_x].index, y=df_left.iloc[::iloc_x].in_a_row, name=\"Left\"))\n",
    "fig.add_trace(go.Scatter(x=df_right.iloc[::iloc_x].index, y=df_right.iloc[::iloc_x].in_a_row, name=\"Right\"))\n",
    "fig.update_layout(title=\"Amount of kmers missed in a row for left and right VLMC (Left = Turkey, Right = human)\", yaxis_title=\"Missed in a row\", xaxis_title='Kmer')\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_left.iloc[::iloc_x].index, y=df_left.in_a_row.rolling(rolling_x).mean().iloc[::iloc_x], name=\"Left\"))\n",
    "fig.add_trace(go.Scatter(x=df_right.iloc[::iloc_x].index, y=df_right.in_a_row.rolling(rolling_x).mean().iloc[::iloc_x], name=\"Right\"))\n",
    "fig.update_layout(title=\"Rolling average of amount of kmers missed in a row for left and right VLMC (Left = Turkey, Right = human)\", yaxis_title=\"Average missed in a row\", xaxis_title='Kmer')\n",
    "fig.show()\n",
    "\n",
    "df_left = pd.read_csv(\"./tmp/distributions/left_distribution_human_to_human.txt\", sep=\",\", header=None)\n",
    "df_right = pd.read_csv(\"./tmp/distributions/right_distribution_human_to_human.txt\", sep=\",\", header=None)\n",
    "df_left.columns = ['match']\n",
    "df_left['not_match'] = df_left.match != 1\n",
    "df_left['in_a_row'] = df_left['not_match'].cumsum()-df_left['not_match'].cumsum().where(~df_left['not_match']).ffill().fillna(0).astype(int)\n",
    "\n",
    "df_right.columns = ['match']\n",
    "df_right['not_match'] = df_right.match != 1\n",
    "df_right['in_a_row'] = df_right['not_match'].cumsum()-df_right['not_match'].cumsum().where(~df_right['not_match']).ffill().fillna(0).astype(int)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_left.iloc[::iloc_x].index, y=df_left.iloc[::iloc_x].in_a_row, name=\"Left\"))\n",
    "fig.add_trace(go.Scatter(x=df_right.iloc[::iloc_x].index, y=df_right.iloc[::iloc_x].in_a_row, name=\"Right\"))\n",
    "fig.update_layout(title=\"Amount of kmers missed in a row for left and right VLMC (Left = Human, Right = human)\", yaxis_title=\"Missed in a row\", xaxis_title='Kmer')\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_left.iloc[::iloc_x].index, y=df_left.in_a_row.rolling(rolling_x).mean().iloc[::iloc_x], name=\"Left\"))\n",
    "fig.add_trace(go.Scatter(x=df_right.iloc[::iloc_x].index, y=df_right.in_a_row.rolling(rolling_x).mean().iloc[::iloc_x], name=\"Right\"))\n",
    "fig.update_layout(title=\"Rolling average of amount of kmers missed in a row for left and right VLMC (Left = Human, Right = human)\", yaxis_title=\"Average missed in a row\", xaxis_title='Kmer')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of matching kmers \n",
    "Once again when comparing two VLMCs, this graph displays the amount of kmers in each VLMC that is matched with the other VLMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left = pd.read_csv(\"./tmp/distributions/left_distribution_turkey_to_human.txt\", sep=\",\", header=None)\n",
    "df_right = pd.read_csv(\"./tmp/distributions/right_distribution_turkey_to_human.txt\", sep=\",\", header=None)\n",
    "df_left.columns = ['match']\n",
    "df_left['ones'] = 1\n",
    "df_left['matchsum'] = df_left.match.cumsum()\n",
    "df_left['prc_matches'] = df_left.matchsum / df_left.ones.cumsum()\n",
    "\n",
    "df_right.columns = ['match']\n",
    "df_right['ones'] = 1\n",
    "df_right['matchsum'] = df_right.match.cumsum()\n",
    "df_right['prc_matches'] = df_right.matchsum / df_right.ones.cumsum()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_left.iloc[::1000].index, y=df_left.iloc[::1000].prc_matches, name=\"Left\"))\n",
    "fig.add_trace(go.Scatter(x=df_right.iloc[::1000].index, y=df_right.iloc[::1000].prc_matches, name=\"Right\"))\n",
    "fig.update_layout(title=\"Percentage of matches (Left = Turkey, Right = Human)\", yaxis_title=\"percentage\", xaxis_title=\"Kmer\")\n",
    "fig.show()\n",
    "\n",
    "df_left = pd.read_csv(\"./tmp/distributions/left_distribution_human_to_human.txt\", sep=\",\", header=None)\n",
    "df_right = pd.read_csv(\"./tmp/distributions/right_distribution_human_to_human.txt\", sep=\",\", header=None)\n",
    "df_left.columns = ['match']\n",
    "df_left['ones'] = 1\n",
    "df_left['matchsum'] = df_left.match.cumsum()\n",
    "df_left['prc_matches'] = df_left.matchsum / df_left.ones.cumsum()\n",
    "\n",
    "df_right.columns = ['match']\n",
    "df_right['ones'] = 1\n",
    "df_right['matchsum'] = df_right.match.cumsum()\n",
    "df_right['prc_matches'] = df_right.matchsum / df_right.ones.cumsum()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_left.iloc[::1000].index, y=df_left.iloc[::1000].prc_matches, name=\"Left\"))\n",
    "fig.add_trace(go.Scatter(x=df_right.iloc[::1000].index, y=df_right.iloc[::1000].prc_matches, name=\"Right\"))\n",
    "fig.update_layout(title=\"Percentage of matches (Left = Human, Right = Human)\", yaxis_title=\"percentage\", xaxis_title=\"Kmer\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting percentage of length of kmers that is filled\n",
    "For each length of a kmer there can at most exist 4 to the power of the length amount of different Kmers. Here we examine how high of a percentage each length has filled for different DNA datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./tmp/length_distributions/kmer_length_distribution.txt\", sep=\",\", header=None)\n",
    "df.columns = ['input']\n",
    "df['vlmc'] = df.input.apply(lambda x : x.split('_')[0])\n",
    "df['kmer_length'] = df.input.apply(lambda x : int(x.split('_')[1]))\n",
    "df['dataset'] = df.input.apply(lambda x : x.split('_')[2])\n",
    "df['max_depth'] = df.input.apply(lambda x : x.split('_')[3].split('.')[0])\n",
    "df.loc[df.max_depth=='10 copy', 'max_depth'] = 10\n",
    "df['max_depth'] = df.max_depth.apply(int)\n",
    "df['min_count'] = df.input.apply(lambda x : int(x.split('_')[4]))\n",
    "df['threshhold'] = df.input.apply(lambda x : float(x.split('_')[5]))\n",
    "df['kmer_count'] = df.input.apply(lambda x : int(x.split('_')[6]))\n",
    "df['length_max_count'] = df.kmer_length.apply(lambda x : 4**x)\n",
    "df['cover_prc'] = df.kmer_count / df.length_max_count\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[x for x in range(0,11)], y=[4**x for x in range(0,11)], name=\"max count\"))\n",
    "fig.add_trace(go.Scatter(x=[x for x in range(0,7)], y=df[df.max_depth==6].groupby('kmer_length')['kmer_count'].mean(), name='max depth 6'))\n",
    "fig.add_trace(go.Scatter(x=[x for x in range(0,9)], y=df[df.max_depth==8].groupby('kmer_length')['kmer_count'].mean(), name='max depth 8'))\n",
    "fig.add_trace(go.Scatter(x=[x for x in range(0,11)], y=df[df.max_depth==10].groupby('kmer_length')['kmer_count'].mean(), name='max depth 10'))\n",
    "fig.show()\n",
    "df_test = df[df.max_depth==10]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_test[df_test.min_count==9]['kmer_length'].unique(), y=df_test[df_test.min_count==9].groupby('kmer_length').cover_prc.mean(), name='min count 9'))\n",
    "fig.add_trace(go.Scatter(x=df_test[df_test.min_count==3]['kmer_length'].unique(), y=df_test[df_test.min_count==3].groupby('kmer_length').cover_prc.mean(), name='min count 3'))\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df[df.max_depth==10]['kmer_length'].unique(), y=df[df.max_depth==10].groupby('kmer_length').cover_prc.mean(), name='max depth 10'))\n",
    "fig.add_trace(go.Scatter(x=df[df.max_depth==8]['kmer_length'].unique(), y=df[df.max_depth==8].groupby('kmer_length').cover_prc.mean(), name='max depth 8'))\n",
    "fig.add_trace(go.Scatter(x=df[df.max_depth==6]['kmer_length'].unique(), y=df[df.max_depth==6].groupby('kmer_length').cover_prc.mean(), name='max depth 8'))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar to above but examining single VLMC with its actual kmer string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human = pd.read_csv(\"./tmp/one_vlmcs_kmer-distribution.txt\", sep=\",\", header=None)\n",
    "df_turkey = pd.read_csv(\"./tmp/another_vlmcs_kmer-distribution.txt\", sep=\",\", header=None)\n",
    "\n",
    "df_human.columns = ['kmer_string']\n",
    "df_turkey.columns = ['kmer_string']\n",
    "\n",
    "df_human['kmer_string'] = df_human.kmer_string.astype(str)\n",
    "df_turkey['kmer_string'] = df_turkey.kmer_string.astype(str)\n",
    "\n",
    "df_human['start_letter'] = df_human.kmer_string.apply(lambda x : x[0])\n",
    "df_turkey['start_letter'] = df_turkey.kmer_string.apply(lambda x : x[0])\n",
    "\n",
    "df_human['kmer_length'] = df_human.kmer_string.apply(lambda x : len(x))\n",
    "df_turkey['kmer_length'] = df_turkey.kmer_string.apply(lambda x : len(x))\n",
    "\n",
    "df_human.sort_values('kmer_length', inplace=True)\n",
    "df_turkey.sort_values('kmer_length', inplace=True)\n",
    "\n",
    "df_human['one'] = 1\n",
    "df_human['new_index'] = df_human.one.cumsum()\n",
    "\n",
    "df_turkey['one'] = 1\n",
    "df_turkey['new_index'] = df_turkey.one.cumsum()\n",
    "\n",
    "fig = px.scatter(x=df_human.iloc[::1000].new_index, y=df_human.iloc[::1000].kmer_length, color=df_human.iloc[::1000].start_letter)\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(x=df_turkey.iloc[::1000].new_index, y=df_turkey.iloc[::1000].kmer_length, color=df_turkey.iloc[::1000].start_letter)\n",
    "fig.show()\n",
    "divlist = []\n",
    "x_list = []\n",
    "for i in range(1,11):\n",
    "    divlist.append(4**i)\n",
    "    x_list.append(i)\n",
    "filled_human = list(df_human.groupby('kmer_length').kmer_length.count() / divlist) \n",
    "filled_turkey = list(df_turkey.groupby('kmer_length').kmer_length.count() / divlist)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_list, y=filled_human, name='human'))\n",
    "fig.add_trace(go.Scatter(x=x_list, y=filled_turkey, name='turkey'))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing shared kmers with two dimensional plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Div by union "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_vlmcs_compared = 200\n",
    "\n",
    "df_human = pd.read_csv(\"./tmp/integer_rep_distributions/human-integer_rep-distribution.txt\", sep=\",\", header=None)\n",
    "df_human.columns = ['input']\n",
    "df_human['vlmc'] = df_human.input.apply(lambda x : int(x.split('_')[0]))\n",
    "df_human['integer_rep'] = df_human.input.apply(lambda x : int(x.split('_')[1]))\n",
    "df_human = df_human[df_human.vlmc < amount_of_vlmcs_compared]\n",
    "\n",
    "df_ecoli = pd.read_csv(\"./tmp/integer_rep_distributions/ecoli-integer_rep-distribution.txt\", sep=\",\", header=None)\n",
    "df_ecoli.columns = ['input']\n",
    "df_ecoli['vlmc'] = df_ecoli.input.apply(lambda x : int(x.split('_')[0]))\n",
    "df_ecoli['integer_rep'] = df_ecoli.input.apply(lambda x : int(x.split('_')[1]))\n",
    "df_ecoli = df_ecoli[df_ecoli.vlmc < amount_of_vlmcs_compared]\n",
    "\n",
    "df_turkey = pd.read_csv(\"./tmp/integer_rep_distributions/turkey-integer_rep-distribution.txt\", sep=\",\", header=None)\n",
    "df_turkey.columns = ['input']\n",
    "df_turkey['vlmc'] = df_turkey.input.apply(lambda x : int(x.split('_')[0]))\n",
    "df_turkey['integer_rep'] = df_turkey.input.apply(lambda x : int(x.split('_')[1]))\n",
    "df_turkey = df_turkey[df_turkey.vlmc < amount_of_vlmcs_compared]\n",
    "\n",
    "df_corn = pd.read_csv(\"./tmp/integer_rep_distributions/corn-integer_rep-distribution.txt\", sep=\",\", header=None)\n",
    "df_corn.columns = ['input']\n",
    "df_corn['vlmc'] = df_corn.input.apply(lambda x : int(x.split('_')[0]))\n",
    "df_corn['integer_rep'] = df_corn.input.apply(lambda x : int(x.split('_')[1]))\n",
    "df_corn = df_corn[df_corn.vlmc < amount_of_vlmcs_compared]\n",
    "\n",
    "comparing = []\n",
    "values  = []\n",
    "\n",
    "for x, vlmc_1 in enumerate(df_human.vlmc.unique()):\n",
    "    for y, vlmc_2 in enumerate(df_human.vlmc.unique()):\n",
    "        if (x < y):\n",
    "            t1 = list(df_human[df_human.vlmc==vlmc_1].integer_rep)\n",
    "            t2 = list(df_human[df_human.vlmc==vlmc_2].integer_rep)\n",
    "            intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "            div_size = len(np.union1d(t1, t2))\n",
    "            comparing.append(\"human to human\")\n",
    "            values.append((intersect_size / div_size) * 100)\n",
    "\n",
    "for x, vlmc_1 in enumerate(df_ecoli.vlmc.unique()):\n",
    "    for y, vlmc_2 in enumerate(df_ecoli.vlmc.unique()):\n",
    "        if (x < y):\n",
    "            t1 = list(df_ecoli[df_ecoli.vlmc==vlmc_1].integer_rep)\n",
    "            t2 = list(df_ecoli[df_ecoli.vlmc==vlmc_2].integer_rep)\n",
    "            intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "            div_size = len(np.union1d(t1, t2))\n",
    "            comparing.append(\"ecoli to ecoli\")\n",
    "            values.append((intersect_size / div_size) * 100)\n",
    "\n",
    "for x, vlmc_1 in enumerate(df_turkey.vlmc.unique()):\n",
    "    for y, vlmc_2 in enumerate(df_turkey.vlmc.unique()):\n",
    "        if (x < y):\n",
    "            t1 = list(df_turkey[df_turkey.vlmc==vlmc_1].integer_rep)\n",
    "            t2 = list(df_turkey[df_turkey.vlmc==vlmc_2].integer_rep)\n",
    "            intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "            div_size = len(np.union1d(t1, t2))\n",
    "            comparing.append(\"turkey to turkey\")\n",
    "            values.append((intersect_size / div_size) * 100)\n",
    "\n",
    "for x, vlmc_1 in enumerate(df_corn.vlmc.unique()):\n",
    "    for y, vlmc_2 in enumerate(df_corn.vlmc.unique()):\n",
    "        if (x < y):\n",
    "            t1 = list(df_corn[df_corn.vlmc==vlmc_1].integer_rep)\n",
    "            t2 = list(df_corn[df_corn.vlmc==vlmc_2].integer_rep)\n",
    "            intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "            div_size = len(np.union1d(t1, t2))\n",
    "            comparing.append(\"corn to corn\")\n",
    "            values.append((intersect_size / div_size) * 100)\n",
    "\n",
    "for x, vlmc_1 in enumerate(df_human.vlmc.unique()):\n",
    "    for y, vlmc_2 in enumerate(df_ecoli.vlmc.unique()):\n",
    "        t1 = list(df_human[df_human.vlmc==vlmc_1].integer_rep)\n",
    "        t2 = list(df_ecoli[df_ecoli.vlmc==vlmc_2].integer_rep)\n",
    "        intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "        div_size = len(np.union1d(t1, t2))\n",
    "        comparing.append(\"human to ecoli\")\n",
    "        values.append((intersect_size / div_size) * 100)\n",
    "\n",
    "for x, vlmc_1 in enumerate(df_human.vlmc.unique()):\n",
    "    for y, vlmc_2 in enumerate(df_turkey.vlmc.unique()):\n",
    "        t1 = list(df_human[df_human.vlmc==vlmc_1].integer_rep)\n",
    "        t2 = list(df_turkey[df_turkey.vlmc==vlmc_2].integer_rep)\n",
    "        intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "        div_size = len(np.union1d(t1, t2))\n",
    "        comparing.append(\"human to turkey\")\n",
    "        values.append((intersect_size / div_size) * 100)\n",
    "\n",
    "for x, vlmc_1 in enumerate(df_human.vlmc.unique()):\n",
    "    for y, vlmc_2 in enumerate(df_corn.vlmc.unique()):\n",
    "        t1 = list(df_human[df_human.vlmc==vlmc_1].integer_rep)\n",
    "        t2 = list(df_corn[df_corn.vlmc==vlmc_2].integer_rep)\n",
    "        intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "        div_size = len(np.union1d(t1, t2))\n",
    "        comparing.append(\"human to corn\")\n",
    "        values.append((intersect_size / div_size) * 100)\n",
    "\n",
    "for x, vlmc_1 in enumerate(df_ecoli.vlmc.unique()):\n",
    "    for y, vlmc_2 in enumerate(df_turkey.vlmc.unique()):\n",
    "        t1 = list(df_ecoli[df_ecoli.vlmc==vlmc_1].integer_rep)\n",
    "        t2 = list(df_turkey[df_turkey.vlmc==vlmc_2].integer_rep)\n",
    "        intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "        div_size = len(np.union1d(t1, t2))\n",
    "        comparing.append(\"ecoli to turkey\")\n",
    "        values.append((intersect_size / div_size) * 100)\n",
    "\n",
    "for x, vlmc_1 in enumerate(df_ecoli.vlmc.unique()):\n",
    "    for y, vlmc_2 in enumerate(df_corn.vlmc.unique()):\n",
    "        t1 = list(df_ecoli[df_ecoli.vlmc==vlmc_1].integer_rep)\n",
    "        t2 = list(df_corn[df_corn.vlmc==vlmc_2].integer_rep)\n",
    "        intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "        div_size = len(np.union1d(t1, t2))\n",
    "        comparing.append(\"ecoli to corn\")\n",
    "        values.append((intersect_size / div_size) * 100)\n",
    "\n",
    "for x, vlmc_1 in enumerate(df_turkey.vlmc.unique()):\n",
    "    for y, vlmc_2 in enumerate(df_corn.vlmc.unique()):\n",
    "        t1 = list(df_turkey[df_turkey.vlmc==vlmc_1].integer_rep)\n",
    "        t2 = list(df_corn[df_corn.vlmc==vlmc_2].integer_rep)\n",
    "        intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "        div_size = len(np.union1d(t1, t2))\n",
    "        comparing.append(\"turkey to corn\")\n",
    "        values.append((intersect_size / div_size) * 100)\n",
    "\n",
    "dict_tmp = {\n",
    "    'comparing' : comparing,\n",
    "    'values' : values\n",
    "}\n",
    "df = pd.DataFrame(dict_tmp)\n",
    "\n",
    "fig = go.Figure()\n",
    "for comp in df.comparing.unique():\n",
    "    fig.add_trace(go.Box(y=df[df.comparing==comp]['values'], name=comp))\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    margin_l=100, margin_r=60, margin_t=60, margin_b=80,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor=\"white\")\n",
    "fig.update_xaxes(title='Data sets',\n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey',\n",
    "    tickfont_size=20,\n",
    "    linewidth=1,\n",
    "    linecolor='LightGrey')\n",
    "fig.update_yaxes(title='Percent of shared K-mers (%)', \n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey', \n",
    "    tickfont_size=20, \n",
    "    showline=True,\n",
    "    linewidth=1)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Div by size of smallest VLMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_integer_rep_distribution(set_size: str):\n",
    "    amount_of_vlmcs_compared = 200\n",
    "    \n",
    "    df_human = pd.read_csv(\"./tmp/integer_rep_distributions/\"+ set_size + \"/human-integer_rep-distribution.txt\", sep=\",\", header=None)\n",
    "    df_human.columns = ['input']\n",
    "    df_human['vlmc'] = df_human.input.apply(lambda x : int(x.split('_')[0]))\n",
    "    df_human['integer_rep'] = df_human.input.apply(lambda x : int(x.split('_')[1]))\n",
    "    df_human = df_human[df_human.vlmc < amount_of_vlmcs_compared]\n",
    "    \n",
    "    df_ecoli = pd.read_csv(\"./tmp/integer_rep_distributions/\"+ set_size + \"/ecoli-integer_rep-distribution.txt\", sep=\",\", header=None)\n",
    "    df_ecoli.columns = ['input']\n",
    "    df_ecoli['vlmc'] = df_ecoli.input.apply(lambda x : int(x.split('_')[0]))\n",
    "    df_ecoli['integer_rep'] = df_ecoli.input.apply(lambda x : int(x.split('_')[1]))\n",
    "    df_ecoli = df_ecoli[df_ecoli.vlmc < amount_of_vlmcs_compared]\n",
    "    \n",
    "    df_turkey = pd.read_csv(\"./tmp/integer_rep_distributions/\"+ set_size + \"/turkey-integer_rep-distribution.txt\", sep=\",\", header=None)\n",
    "    df_turkey.columns = ['input']\n",
    "    df_turkey['vlmc'] = df_turkey.input.apply(lambda x : int(x.split('_')[0]))\n",
    "    df_turkey['integer_rep'] = df_turkey.input.apply(lambda x : int(x.split('_')[1]))\n",
    "    df_turkey = df_turkey[df_turkey.vlmc < amount_of_vlmcs_compared]\n",
    "    \n",
    "    df_corn = pd.read_csv(\"./tmp/integer_rep_distributions/\"+ set_size + \"/corn-integer_rep-distribution.txt\", sep=\",\", header=None)\n",
    "    df_corn.columns = ['input']\n",
    "    df_corn['vlmc'] = df_corn.input.apply(lambda x : int(x.split('_')[0]))\n",
    "    df_corn['integer_rep'] = df_corn.input.apply(lambda x : int(x.split('_')[1]))\n",
    "    df_corn = df_corn[df_corn.vlmc < amount_of_vlmcs_compared]\n",
    "    \n",
    "    comparing = []\n",
    "    values  = []\n",
    "    \n",
    "    for x, vlmc_1 in enumerate(df_human.vlmc.unique()):\n",
    "        for y, vlmc_2 in enumerate(df_human.vlmc.unique()):\n",
    "            if (x < y):\n",
    "                t1 = (df_human[df_human.vlmc==vlmc_1].integer_rep).to_numpy()\n",
    "                t2 = (df_human[df_human.vlmc==vlmc_2].integer_rep).to_numpy()\n",
    "                intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "                div_size = min(len(t1), len(t2))\n",
    "                comparing.append(\"human to human\")\n",
    "                values.append((intersect_size / div_size) * 100)\n",
    "    \n",
    "    for x, vlmc_1 in enumerate(df_ecoli.vlmc.unique()):\n",
    "        for y, vlmc_2 in enumerate(df_ecoli.vlmc.unique()):\n",
    "            if (x < y):\n",
    "                t1 = (df_ecoli[df_ecoli.vlmc==vlmc_1].integer_rep).to_numpy()\n",
    "                t2 = (df_ecoli[df_ecoli.vlmc==vlmc_2].integer_rep).to_numpy()\n",
    "                intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "                div_size = min(len(t1), len(t2))\n",
    "                comparing.append(\"ecoli to ecoli\")\n",
    "                values.append((intersect_size / div_size) * 100)\n",
    "    \n",
    "    for x, vlmc_1 in enumerate(df_turkey.vlmc.unique()):\n",
    "        for y, vlmc_2 in enumerate(df_turkey.vlmc.unique()):\n",
    "            if (x < y):\n",
    "                t1 = (df_turkey[df_turkey.vlmc==vlmc_1].integer_rep).to_numpy()\n",
    "                t2 = (df_turkey[df_turkey.vlmc==vlmc_2].integer_rep).to_numpy()\n",
    "                intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "                div_size = min(len(t1), len(t2))\n",
    "                comparing.append(\"turkey to turkey\")\n",
    "                values.append((intersect_size / div_size) * 100)\n",
    "    \n",
    "    for x, vlmc_1 in enumerate(df_corn.vlmc.unique()):\n",
    "        for y, vlmc_2 in enumerate(df_corn.vlmc.unique()):\n",
    "            if (x < y):\n",
    "                t1 = (df_corn[df_corn.vlmc==vlmc_1].integer_rep).to_numpy()\n",
    "                t2 = (df_corn[df_corn.vlmc==vlmc_2].integer_rep).to_numpy()\n",
    "                intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "                div_size = min(len(t1), len(t2))\n",
    "                comparing.append(\"corn to corn\")\n",
    "                values.append((intersect_size / div_size) * 100)\n",
    "    \n",
    "    for x, vlmc_1 in enumerate(df_human.vlmc.unique()):\n",
    "        for y, vlmc_2 in enumerate(df_ecoli.vlmc.unique()):\n",
    "            t1 = (df_human[df_human.vlmc==vlmc_1].integer_rep).to_numpy()\n",
    "            t2 = (df_ecoli[df_ecoli.vlmc==vlmc_2].integer_rep).to_numpy()\n",
    "            intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "            div_size = min(len(t1), len(t2))\n",
    "            comparing.append(\"human to ecoli\")\n",
    "            values.append((intersect_size / div_size) * 100)\n",
    "    \n",
    "    for x, vlmc_1 in enumerate(df_human.vlmc.unique()):\n",
    "        for y, vlmc_2 in enumerate(df_turkey.vlmc.unique()):\n",
    "            t1 = (df_human[df_human.vlmc==vlmc_1].integer_rep).to_numpy()\n",
    "            t2 = (df_turkey[df_turkey.vlmc==vlmc_2].integer_rep).to_numpy()\n",
    "            intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "            div_size = min(len(t1), len(t2))\n",
    "            comparing.append(\"human to turkey\")\n",
    "            values.append((intersect_size / div_size) * 100)\n",
    "    \n",
    "    for x, vlmc_1 in enumerate(df_human.vlmc.unique()):\n",
    "        for y, vlmc_2 in enumerate(df_corn.vlmc.unique()):\n",
    "            t1 = (df_human[df_human.vlmc==vlmc_1].integer_rep).to_numpy()\n",
    "            t2 = (df_corn[df_corn.vlmc==vlmc_2].integer_rep).to_numpy()\n",
    "            intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "            div_size = min(len(t1), len(t2))\n",
    "            comparing.append(\"human to corn\")\n",
    "            values.append((intersect_size / div_size) * 100)\n",
    "    \n",
    "    for x, vlmc_1 in enumerate(df_ecoli.vlmc.unique()):\n",
    "        for y, vlmc_2 in enumerate(df_turkey.vlmc.unique()):\n",
    "            t1 = (df_ecoli[df_ecoli.vlmc==vlmc_1].integer_rep).to_numpy()\n",
    "            t2 = (df_turkey[df_turkey.vlmc==vlmc_2].integer_rep).to_numpy()\n",
    "            intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "            div_size = min(len(t1), len(t2))\n",
    "            comparing.append(\"ecoli to turkey\")\n",
    "            values.append((intersect_size / div_size) * 100)\n",
    "    \n",
    "    for x, vlmc_1 in enumerate(df_ecoli.vlmc.unique()):\n",
    "        for y, vlmc_2 in enumerate(df_corn.vlmc.unique()):\n",
    "            t1 = (df_ecoli[df_ecoli.vlmc==vlmc_1].integer_rep).to_numpy()\n",
    "            t2 = (df_corn[df_corn.vlmc==vlmc_2].integer_rep).to_numpy()\n",
    "            intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "            div_size = min(len(t1), len(t2))\n",
    "            comparing.append(\"ecoli to corn\")\n",
    "            values.append((intersect_size / div_size) * 100)\n",
    "    \n",
    "    for x, vlmc_1 in enumerate(df_turkey.vlmc.unique()):\n",
    "        for y, vlmc_2 in enumerate(df_corn.vlmc.unique()):\n",
    "            t1 = (df_turkey[df_turkey.vlmc==vlmc_1].integer_rep).to_numpy()\n",
    "            t2 = (df_corn[df_corn.vlmc==vlmc_2].integer_rep).to_numpy()\n",
    "            intersect_size = len(np.intersect1d(t1, t2, assume_unique=True))\n",
    "            div_size = min(len(t1), len(t2))\n",
    "            comparing.append(\"turkey to corn\")\n",
    "            values.append((intersect_size / div_size) * 100)\n",
    "    \n",
    "    dict_tmp = {\n",
    "        'comparing' : comparing,\n",
    "        'values' : values\n",
    "    }\n",
    "    df = pd.DataFrame(dict_tmp)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for comp in df.comparing.unique():\n",
    "        fig.add_trace(go.Box(y=df[df.comparing==comp]['values'], name=comp))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        margin_l=100, margin_r=60, margin_t=60, margin_b=80,\n",
    "        height=600,\n",
    "        width=1200,\n",
    "        plot_bgcolor=\"white\")\n",
    "    fig.update_xaxes(title='Data sets',\n",
    "        title_font_size=24,\n",
    "        gridcolor='LightGrey',\n",
    "        tickfont_size=20,\n",
    "        linewidth=1,\n",
    "        linecolor='LightGrey')\n",
    "    fig.update_yaxes(title='Percent of shared K-mers (%)', \n",
    "        title_font_size=24,\n",
    "        gridcolor='LightGrey', \n",
    "        tickfont_size=20, \n",
    "        showline=True,\n",
    "        linewidth=1)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_integer_rep_distribution(\"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_integer_rep_distribution(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_integer_rep_distribution(\"large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./tmp/integer_rep_distributions/integer_rep-distribution.txt\", sep=\",\", header=None)  \n",
    "df.columns = ['input']\n",
    "df['left_id'] = df.input.apply(lambda x : x.split('_')[0])\n",
    "df['left_size'] = df.input.apply(lambda x : x.split('_')[1])\n",
    "df['right_id'] = df.input.apply(lambda x : x.split('_')[3])\n",
    "df['right_size'] = df.input.apply(lambda x : x.split('_')[4])\n",
    "df['comparing'] = df['left_id'] + \"_\" + df['right_id']\n",
    "df['vlmc_id'] = df.input.apply(lambda x : x.split('_')[2] + \"_\" + x.split('_')[5])\n",
    "df['intersection_count'] = df.input.apply(lambda x : int(x.split('_')[6]))\n",
    "df['left_count'] = df.input.apply(lambda x : int(x.split('_')[7]))\n",
    "df['right_count'] = df.input.apply(lambda x : int(x.split('_')[8]))\n",
    "df['min_count'] = df[['left_count','right_count']].min(axis=1)\n",
    "df['percentage'] = (df['intersection_count'] / df['min_count']) * 100\n",
    "\n",
    "print(\"Small\")\n",
    "df_small = df[(df.left_size==\"small\") & (df.right_size==\"small\")]\n",
    "fig = go.Figure()\n",
    "for comp in df_small.comparing.unique():\n",
    "    fig.add_trace(go.Box(y=df_small[df_small.comparing==comp]['percentage'], name=comp))\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    margin_l=100, margin_r=60, margin_t=60, margin_b=80,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor=\"white\")\n",
    "fig.update_xaxes(title='Data sets',\n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey',\n",
    "    tickfont_size=20,\n",
    "    linewidth=1,\n",
    "    linecolor='LightGrey')\n",
    "fig.update_yaxes(title='Percent of shared K-mers (%)', \n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey', \n",
    "    tickfont_size=20, \n",
    "    showline=True,\n",
    "    linewidth=1)\n",
    "fig.show()\n",
    "\n",
    "print(\"Medium\")\n",
    "df_medium = df[(df.left_size==\"medium\") & (df.right_size==\"medium\")]\n",
    "fig = go.Figure()\n",
    "for comp in df_medium.comparing.unique():\n",
    "    fig.add_trace(go.Box(y=df_medium[df_medium.comparing==comp]['percentage'], name=comp))\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    margin_l=100, margin_r=60, margin_t=60, margin_b=80,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor=\"white\")\n",
    "fig.update_xaxes(title='Data sets',\n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey',\n",
    "    tickfont_size=20,\n",
    "    linewidth=1,\n",
    "    linecolor='LightGrey')\n",
    "fig.update_yaxes(title='Percent of shared K-mers (%)', \n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey', \n",
    "    tickfont_size=20, \n",
    "    showline=True,\n",
    "    linewidth=1)\n",
    "fig.show()\n",
    "\n",
    "print(\"Large\")\n",
    "df_large = df[(df.left_size==\"large\") & (df.right_size==\"large\")]\n",
    "fig = go.Figure()\n",
    "for comp in df_large.comparing.unique():\n",
    "    fig.add_trace(go.Box(y=df_large[df_large.comparing==comp]['percentage'], name=comp))\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    margin_l=100, margin_r=60, margin_t=60, margin_b=80,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor=\"white\")\n",
    "fig.update_xaxes(title='Data sets',\n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey',\n",
    "    tickfont_size=20,\n",
    "    linewidth=1,\n",
    "    linecolor='LightGrey')\n",
    "fig.update_yaxes(title='Percent of shared K-mers (%)', \n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey', \n",
    "    tickfont_size=20, \n",
    "    showline=True,\n",
    "    linewidth=1)\n",
    "fig.show()\n",
    "\n",
    "print(\"Diverse\")\n",
    "df_diverse = df[(df.left_size==\"diverse\") & (df.right_size==\"diverse\")]\n",
    "fig = go.Figure()\n",
    "for comp in df_diverse.comparing.unique():\n",
    "    fig.add_trace(go.Box(y=df_diverse[df_diverse.comparing==comp]['percentage'], name=comp))\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    margin_l=100, margin_r=60, margin_t=60, margin_b=80,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor=\"white\")\n",
    "fig.update_xaxes(title='Data sets',\n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey',\n",
    "    tickfont_size=20,\n",
    "    linewidth=1,\n",
    "    linecolor='LightGrey')\n",
    "fig.update_yaxes(title='Percent of shared K-mers (%)', \n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey', \n",
    "    tickfont_size=20, \n",
    "    showline=True,\n",
    "    linewidth=1)\n",
    "fig.show()\n",
    "\n",
    "print(\"Mega\")\n",
    "df_mega = df[(df.left_size==\"mega\") & (df.right_size==\"mega\")]\n",
    "fig = go.Figure()\n",
    "for comp in df_mega.comparing.unique():\n",
    "    fig.add_trace(go.Box(y=df_mega[df_mega.comparing==comp]['percentage'], name=comp))\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    margin_l=100, margin_r=60, margin_t=60, margin_b=80,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor=\"white\")\n",
    "fig.update_xaxes(title='Data sets',\n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey',\n",
    "    tickfont_size=20,\n",
    "    linewidth=1,\n",
    "    linecolor='LightGrey')\n",
    "fig.update_yaxes(title='Percent of shared K-mers (%)', \n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey', \n",
    "    tickfont_size=20, \n",
    "    showline=True,\n",
    "    linewidth=1)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missed k-mers in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"tmp/integer_rep_distributions/missed-in-a-row/joels/\"\n",
    "    \n",
    "all_df = []\n",
    "for file in os.listdir(directory):\n",
    "    df_tmp = pd.read_csv(directory + file, sep=\",\", header=None)\n",
    "    df_tmp.columns = ['input']\n",
    "    df_tmp['left_dataset'] = df_tmp.input.apply(lambda x : x.split('_')[0]) \n",
    "    df_tmp['left_vlmc'] = df_tmp.input.apply(lambda x : int(x.split('_')[1])) \n",
    "    df_tmp['right_dataset'] = df_tmp.input.apply(lambda x : x.split('_')[2])\n",
    "    df_tmp['right_vlmc'] = df_tmp.input.apply(lambda x : int(x.split('_')[3])) \n",
    "    df_tmp['left_size'] = df_tmp.input.apply(lambda x : int(x.split('_')[4])) \n",
    "    df_tmp['right_size'] = df_tmp.input.apply(lambda x : int(x.split('_')[5])) \n",
    "    df_tmp['misses'] = df_tmp.input.apply(lambda x : int(x.split('_')[6])) \n",
    "    df_tmp['matched'] = 1\n",
    "    df_tmp['matched'] = df_tmp.matched.cumsum()\n",
    "    all_df.append(df_tmp)\n",
    "\n",
    "df = pd.concat(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_showing = 1004\n",
    "rolling = 500 \n",
    "\n",
    "fig = go.Figure()\n",
    "left_v = 2\n",
    "for right_v in df.right_vlmc.unique():\n",
    "    df_tmp = df[(df.left_vlmc==left_v) & (df.right_vlmc==right_v)]\n",
    "    print(right_v)\n",
    "    print(df_tmp.right_size.head(1))\n",
    "    fig.add_trace(go.Scatter(x=df_tmp.matched.iloc[::skip_showing], y=df_tmp.misses.rolling(rolling).mean().iloc[::skip_showing], name=str(left_v) + str(right_v)))\n",
    "fig.update_layout(\n",
    "    showlegend=True,\n",
    "    margin_l=100, margin_r=60, margin_t=60, margin_b=80,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor=\"white\")\n",
    "fig.update_xaxes(title='Index',\n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey',\n",
    "    tickfont_size=20,\n",
    "    linewidth=1,\n",
    "    linecolor='LightGrey')\n",
    "fig.update_yaxes(title='Rolling mean misses in a row', \n",
    "    title_font_size=24,\n",
    "    gridcolor='LightGrey', \n",
    "    tickfont_size=20, \n",
    "    showline=True,\n",
    "    linewidth=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"tmp/integer_rep_distributions/missed-in-a-row/mega/\"\n",
    "    \n",
    "all_df = []\n",
    "for file in os.listdir(directory):\n",
    "    df_tmp = pd.read_csv(directory + file, sep=\",\", header=None)\n",
    "    df_tmp.columns = ['input']\n",
    "    df_tmp['left_dataset'] = df_tmp.input.apply(lambda x : x.split('_')[0]) \n",
    "    df_tmp['left_vlmc'] = df_tmp.input.apply(lambda x : int(x.split('_')[1])) \n",
    "    df_tmp['right_dataset'] = df_tmp.input.apply(lambda x : x.split('_')[2])\n",
    "    df_tmp['right_vlmc'] = df_tmp.input.apply(lambda x : int(x.split('_')[3])) \n",
    "    df_tmp['left_size'] = df_tmp.input.apply(lambda x : int(x.split('_')[4])) \n",
    "    df_tmp['right_size'] = df_tmp.input.apply(lambda x : int(x.split('_')[5])) \n",
    "    df_tmp['hits'] = df_tmp.input.apply(lambda x : int(x.split('_')[6])) \n",
    "    df_tmp['misses'] = df_tmp.input.apply(lambda x : int(x.split('_')[7])) \n",
    "    df_tmp['matched'] = 1\n",
    "    df_tmp['matched'] = df_tmp.matched.cumsum()\n",
    "    if df_tmp.left_vlmc.iloc[0]!=df_tmp.right_vlmc.iloc[0]:\n",
    "        all_df.append(df_tmp)\n",
    "\n",
    "df = pd.concat(all_df)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tot'] = df.hits + df.misses \n",
    "df['prc'] = df.misses / df.tot\n",
    "\n",
    "df['prc'] = df.prc \n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "datasets = ['human_human', 'human_turkey', 'human_corn', 'turkey_turkey', 'turkey_corn', 'corn_corn']\n",
    "\n",
    "for dataset in datasets:\n",
    "    left_d = dataset.split('_')[0]\n",
    "    right_d = dataset.split('_')[1]\n",
    "    df_tmp = df[(df.left_dataset==left_d) & (df.right_dataset==right_d)]\n",
    "    df_tmp['mean'] = df_tmp.groupby('matched').prc.transform('mean')\n",
    "    df_tmp['std'] = df_tmp.groupby('matched').prc.transform('std')\n",
    "    df_tmp['std_above'] = df_tmp['mean'] + df_tmp['std']\n",
    "    df_tmp['std_below'] = df_tmp['mean'] - df_tmp['std']\n",
    "    df_tmp = df_tmp[(df_tmp.left_vlmc==1) & (df_tmp.right_vlmc==0)]\n",
    "    fig.add_trace(go.Scatter(x=df_tmp['matched'] * 5, y=df_tmp['mean'], mode='lines',\n",
    "                         # line=dict(color=line_color),\n",
    "                         name=left_d + \" to \" + right_d))\n",
    "\n",
    "plot_color = \"#636EFA\"  # \"rgb(136, 204, 238)\"\n",
    "line_color = \"#636EFA\" # \"rgb(51, 34, 136)\"\n",
    "second_line_color = \"#FECB52\" # \"rgb(102, 17, 0)\"\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=df_tmp['matched'], y=df_tmp['std_above'], mode='lines',\n",
    "#                                      line=dict(color=plot_color,width =0.1),\n",
    "#                                      name='upper bound', showlegend=False))\n",
    "# \n",
    "# fig.add_trace(go.Scatter(x=df_tmp['matched'], y=df_tmp['mean'], mode='lines',\n",
    "#                          line=dict(color=plot_color),\n",
    "#                          fill='tonexty',\n",
    "#                          name='mean', showlegend=False))\n",
    "# \n",
    "# fig.add_trace(go.Scatter(x=df_tmp['matched'], y=df_tmp['std_below'], mode='lines',\n",
    "#                          line=dict(color=plot_color, width =0.1),\n",
    "#                          fill='tonexty',\n",
    "#                          name='Standard deviation'))\n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"h\",\n",
    "    yanchor=\"bottom\",\n",
    "    font_size=24,\n",
    "    y=1.08),\n",
    "    margin=dict(l=100, r=60, t=60, b=80),\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor=\"white\",\n",
    "    showlegend=True)\n",
    "\n",
    "fig.update_xaxes(title=\"Slice of VLMC (%)\", \n",
    "                gridcolor='LightGrey', \n",
    "                tickfont_size=20, \n",
    "                showline=True, \n",
    "                linewidth=1, \n",
    "                linecolor='LightGrey', \n",
    "                title_font_size=24)\n",
    "\n",
    "fig.update_yaxes(title=\"Percentage of misses (%)\", \n",
    "                 gridcolor='LightGrey', \n",
    "                 tickfont_size=20, \n",
    "                 showline=True, \n",
    "                 linewidth=1, \n",
    "                 linecolor='LightGrey', \n",
    "                 title_font_size=24)\n",
    "\n",
    "fig.write_image(\"images/prc_missed.pdf\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tot'] = df.hits + df.misses \n",
    "df['prc'] = df.misses / df.tot\n",
    "\n",
    "df['prc'] = df.prc \n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "datasets = ['human_human', 'human_turkey', 'human_corn', 'turkey_turkey', 'turkey_corn', 'corn_corn']\n",
    "\n",
    "for dataset in datasets:\n",
    "    left_d = dataset.split('_')[0]\n",
    "    right_d = dataset.split('_')[1]\n",
    "    df_tmp = df[(df.left_dataset==left_d) & (df.right_dataset==right_d)]\n",
    "    df_tmp['mean'] = df_tmp.groupby('matched').misses.transform('mean')\n",
    "    df_tmp['std'] = df_tmp.groupby('matched').misses.transform('std')\n",
    "    df_tmp['std_above'] = df_tmp['mean'] + df_tmp['std']\n",
    "    df_tmp['std_below'] = df_tmp['mean'] - df_tmp['std']\n",
    "    df_tmp = df_tmp[(df_tmp.left_vlmc==1) & (df_tmp.right_vlmc==0)]\n",
    "    fig.add_trace(go.Scatter(x=df_tmp['matched'] * 5, y=df_tmp['mean'], mode='lines',\n",
    "                         # line=dict(color=line_color),\n",
    "                         name=left_d + \" to \" + right_d))\n",
    "\n",
    "plot_color = \"#636EFA\"  # \"rgb(136, 204, 238)\"\n",
    "line_color = \"#636EFA\" # \"rgb(51, 34, 136)\"\n",
    "second_line_color = \"#FECB52\" # \"rgb(102, 17, 0)\"\n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"h\",\n",
    "    yanchor=\"bottom\",\n",
    "    font_size=24,\n",
    "    y=1.08),\n",
    "    margin=dict(l=100, r=60, t=60, b=80),\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor=\"white\",\n",
    "    showlegend=True)\n",
    "\n",
    "fig.update_xaxes(title=\"Slice of VLMC (%)\", \n",
    "                gridcolor='LightGrey', \n",
    "                tickfont_size=20, \n",
    "                showline=True, \n",
    "                linewidth=1, \n",
    "                linecolor='LightGrey', \n",
    "                title_font_size=24)\n",
    "\n",
    "fig.update_yaxes(title=\"Amount of misses\", \n",
    "                 gridcolor='LightGrey', \n",
    "                 tickfont_size=20, \n",
    "                 showline=True, \n",
    "                 linewidth=1, \n",
    "                 linecolor='LightGrey', \n",
    "                 title_font_size=24)\n",
    "\n",
    "fig.write_image(\"images/amount_misses.pdf\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iloc_x = 1000\n",
    "rolling_x = 10000\n",
    "directory = \"tmp/integer_rep_distributions/missed-in-a-row/actual-counts/\"\n",
    "\n",
    "alldirs = []\n",
    "file_nr = 0\n",
    "for file in os.listdir(directory):\n",
    "    df_tmp = pd.read_csv(directory + file, sep=\",\", header=None)\n",
    "    df_tmp.columns = ['not_match']\n",
    "    df_tmp['not_match'] = df_tmp.not_match.apply(bool)\n",
    "    df_tmp['in_a_row'] = df_tmp['not_match'].cumsum()-df_tmp['not_match'].cumsum().where(~df_tmp['not_match']).ffill().fillna(0).astype(int)\n",
    "    df_tmp['roll'] = df_tmp.in_a_row.rolling(rolling_x).mean()\n",
    "    df_tmp['roll'] = df_tmp.roll.fillna(0) \n",
    "    df_tmp['dataset'] = file.split('_')[0]\n",
    "    df_tmp['idx'] = 1\n",
    "    df_tmp['idx'] = df_tmp.idx.cumsum()\n",
    "    df_tmp['file_nr'] = file_nr \n",
    "    df_tmp = df_tmp.iloc[::iloc_x]\n",
    "    alldirs.append(df_tmp)\n",
    "    file_nr += 1 \n",
    "\n",
    "df = pd.concat(alldirs)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.dataset=='human'].file_nr.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "directory = \"tmp/integer_rep_distributions/missed-in-a-row/actual-counts/\"\n",
    "    \n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "df_turkey = df[df.dataset=='turkey']\n",
    "df_human = df[df.dataset=='human']\n",
    "\n",
    "df_turkey['mean'] = df_turkey.groupby('idx').roll.transform('mean')\n",
    "df_turkey['std'] = df_turkey.groupby('idx').roll.transform('std')\n",
    "df_turkey['std_above'] = df_turkey['mean'] + df_turkey['std']\n",
    "df_turkey['std_below'] = df_turkey['mean'] - df_turkey['std']\n",
    "\n",
    "df_human['mean'] = df_human.groupby('idx').roll.transform('mean')\n",
    "df_human['std'] = df_human.groupby('idx').roll.transform('std')\n",
    "df_human['std_above'] = df_human['mean'] + df_human['std']\n",
    "df_human['std_below'] = df_human['mean'] - df_human['std']\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_turkey[df_turkey.file_nr==0].idx, y=df_turkey[df_turkey.file_nr==0]['mean'], name='turkey to corn'))\n",
    "fig.add_trace(go.Scatter(x=df_human[df_human.file_nr==3].idx, y=df_human[df_human.file_nr==3]['mean'], name='human to corn'), secondary_y=False)\n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"h\",\n",
    "    yanchor=\"bottom\",\n",
    "    font_size=24,\n",
    "    y=1.08),\n",
    "    margin=dict(l=100, r=60, t=60, b=80),\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor=\"white\",\n",
    "    showlegend=True)\n",
    "\n",
    "fig.update_xaxes(title=\"Slice of VLMC (%)\", \n",
    "                gridcolor='LightGrey', \n",
    "                tickfont_size=20, \n",
    "                showline=True, \n",
    "                linewidth=1, \n",
    "                linecolor='LightGrey', \n",
    "                title_font_size=24)\n",
    "\n",
    "fig.update_yaxes(title=\"Amount of misses\", \n",
    "                 gridcolor='LightGrey', \n",
    "                 tickfont_size=20, \n",
    "                 showline=True, \n",
    "                 linewidth=1, \n",
    "                 linecolor='LightGrey', \n",
    "                 title_font_size=24,\n",
    "                 secondary_y=False)\n",
    "\n",
    "fig.update_yaxes(title=\"Amount of misses\", \n",
    "                 tickfont_size=20, \n",
    "                 showline=True, \n",
    "                 linewidth=1, \n",
    "                 linecolor='LightGrey', \n",
    "                 title_font_size=24,\n",
    "                 secondary_y=True)\n",
    "\n",
    "fig.write_image(\"images/missed-in-a-row.pdf\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"tmp/integer_rep_distributions/kmer-length-filled/\"\n",
    "\n",
    "alldirs = []\n",
    "for file in os.listdir(directory):\n",
    "    df_tmp = pd.read_csv(directory + file, sep=\",\", header=None)\n",
    "    df_tmp.columns = ['input']\n",
    "    df_tmp['species'] = file.split('_')[0].replace('\"', '')\n",
    "    df_tmp['size'] = file.split('_')[1].split('.')[0].replace('\"', '')\n",
    "    df_tmp['kmer_length'] = df_tmp['input'].apply(lambda x : int(x.split('_')[0]))\n",
    "    df_tmp['length_count'] = df_tmp['input'].apply(lambda x : int(x.split('_')[1]))\n",
    "    alldirs.append(df_tmp)\n",
    "\n",
    "df = pd.concat(alldirs)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#00CC96', '#636EFA', '#FECB52', '#EF553B']\n",
    "df['species'] = df.species.apply(lambda x : x.replace('ecoli', 'virus'))\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True, subplot_titles=(\"Small\", \"Medium\", \"Large\"), \n",
    "                    horizontal_spacing= 0.02, vertical_spacing= 0.06,\n",
    "                    x_title=\"K-mer length\",\n",
    "                    y_title=\"Percentage filled (%)\")\n",
    "for i, species in enumerate(df.species.unique()):\n",
    "    for size in df['size'].unique():\n",
    "        if size == 'small':\n",
    "            df_tmp = df[(df.species==species) & (df['size']==size) & (df['kmer_length'] < 7)]\n",
    "            fig.add_trace(go.Scatter(x=df_tmp.kmer_length, y=(df_tmp.length_count / (4**df_tmp.kmer_length)) * 100, name=species, line_color=colors[i]), col=1, row=1)\n",
    "        elif size == 'medium':\n",
    "            df_tmp = df[(df.species==species) & (df['size']==size) & (df['kmer_length'] < 9)]\n",
    "            fig.add_trace(go.Scatter(x=df_tmp.kmer_length, y=(df_tmp.length_count / (4**df_tmp.kmer_length)) * 100, name=species, line_color=colors[i], showlegend = False), col=1, row=2)\n",
    "        else: \n",
    "            df_tmp = df[(df.species==species) & (df['size']==size) & (df['kmer_length'] < 11)]\n",
    "            fig.add_trace(go.Scatter(x=df_tmp.kmer_length, y=(df_tmp.length_count / (4**df_tmp.kmer_length)) * 100, name=species, line_color=colors[i], showlegend = False), col=1, row=3)\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"h\",\n",
    "    yanchor=\"bottom\",\n",
    "    font_size=24,\n",
    "    y=1.08),\n",
    "    margin=dict(l=100, r=60, t=60, b=80),\n",
    "    height=800,\n",
    "    plot_bgcolor=\"white\")\n",
    "\n",
    "fig.update_xaxes(gridcolor='LightGrey', showline=True, linewidth=1, linecolor='LightGrey', tickfont_size=20)\n",
    "fig.update_yaxes(gridcolor='LightGrey', showline=True, linewidth=1, linecolor='LightGrey', tickfont_size=20)\n",
    "\n",
    "fig.layout.annotations[0][\"font\"] = {'size': 24}\n",
    "fig.layout.annotations[1][\"font\"] = {'size': 24}\n",
    "fig.layout.annotations[2][\"font\"] = {'size': 24}\n",
    "\n",
    "fig.layout.annotations[3][\"font\"] = {'size': 24}\n",
    "fig.layout.annotations[4][\"font\"] = {'size': 24}\n",
    "\n",
    "fig.write_image(\"images/percentage-kmer-length-filled.pdf\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_sets = ['human to human', 'human to turkey', 'human to corn', 'turkey to turkey', 'turkey to corn', 'corn to corn', 'ecoli to ecoli']\n",
    "\n",
    "df = pd.read_csv(\"tmp/integer_rep_distributions/shared-kmers/distribution_small.txt\", sep=\",\", header=None)\n",
    "df.columns = ['input']\n",
    "\n",
    "df['dataset'] = df.input.apply(lambda x : x.split('_')[0] + \" to \" + x.split('_')[2])\n",
    "df['dataset_bool'] = df.dataset.apply(lambda x : x in keep_sets)\n",
    "df['left_vlmc'] = df.input.apply(lambda x : int(x.split('_')[1]))\n",
    "df['right_vlmc'] = df.input.apply(lambda x : int(x.split('_')[3]))\n",
    "df['hits'] = df.input.apply(lambda x : int(x.split('_')[4]))\n",
    "df['miss_left'] = df.input.apply(lambda x : int(x.split('_')[5]))\n",
    "df['miss_right'] = df.input.apply(lambda x : int(x.split('_')[6]))\n",
    "df['size_left'] = df.input.apply(lambda x : int(x.split('_')[7]))\n",
    "df['size_right'] = df.input.apply(lambda x : int(x.split('_')[8]))\n",
    "df['skipped_left'] = df.input.apply(lambda x : int(x.split('_')[9]))\n",
    "df['skipped_right'] = df.input.apply(lambda x : int(x.split('_')[10]))\n",
    "\n",
    "df['mean_hits'] = df.groupby(['dataset'])['hits'].transform('mean')\n",
    "df['mean_miss_left'] = df.groupby(['dataset'])['miss_left'].transform('mean')\n",
    "df['mean_miss_right'] = df.groupby(['dataset'])['miss_right'].transform('mean')\n",
    "df['mean_misses'] = df.mean_miss_left + df.mean_miss_right \n",
    "df['mean_size_left'] = df.groupby(['dataset'])['size_left'].transform('mean')\n",
    "df['mean_size_right'] = df.groupby(['dataset'])['size_right'].transform('mean')\n",
    "df['mean_skipped_left'] = df.groupby(['dataset'])['skipped_left'].transform('mean')\n",
    "df['mean_skipped_right'] = df.groupby(['dataset'])['skipped_right'].transform('mean')\n",
    "df['mean_skipped'] = df.mean_skipped_left + df.mean_skipped_right \n",
    "\n",
    "df['total_sum'] = df.mean_hits + df.mean_miss_left + df.mean_miss_right + df.mean_skipped_left + df.mean_skipped_right \n",
    "\n",
    "df['right_idx'] = 1\n",
    "df['right_vlmc_cumsum'] = df.groupby('dataset')['right_idx'].transform('cumsum')\n",
    "\n",
    "df_small = df[(df.left_vlmc==0) & (df.right_vlmc_cumsum==1) & (df.dataset_bool==True)]\n",
    "\n",
    "df = pd.read_csv(\"tmp/integer_rep_distributions/shared-kmers/distribution_medium.txt\", sep=\",\", header=None)\n",
    "df.columns = ['input']\n",
    "\n",
    "df['dataset'] = df.input.apply(lambda x : x.split('_')[0] + \" to \" + x.split('_')[2])\n",
    "df['dataset_bool'] = df.dataset.apply(lambda x : x in keep_sets)\n",
    "df['left_vlmc'] = df.input.apply(lambda x : int(x.split('_')[1]))\n",
    "df['right_vlmc'] = df.input.apply(lambda x : int(x.split('_')[3]))\n",
    "df['hits'] = df.input.apply(lambda x : int(x.split('_')[4]))\n",
    "df['miss_left'] = df.input.apply(lambda x : int(x.split('_')[5]))\n",
    "df['miss_right'] = df.input.apply(lambda x : int(x.split('_')[6]))\n",
    "df['size_left'] = df.input.apply(lambda x : int(x.split('_')[7]))\n",
    "df['size_right'] = df.input.apply(lambda x : int(x.split('_')[8]))\n",
    "df['skipped_left'] = df.input.apply(lambda x : int(x.split('_')[9]))\n",
    "df['skipped_right'] = df.input.apply(lambda x : int(x.split('_')[10]))\n",
    "\n",
    "df['mean_hits'] = df.groupby(['dataset'])['hits'].transform('mean')\n",
    "df['mean_miss_left'] = df.groupby(['dataset'])['miss_left'].transform('mean')\n",
    "df['mean_miss_right'] = df.groupby(['dataset'])['miss_right'].transform('mean')\n",
    "df['mean_misses'] = df.mean_miss_left + df.mean_miss_right \n",
    "df['mean_size_left'] = df.groupby(['dataset'])['size_left'].transform('mean')\n",
    "df['mean_size_right'] = df.groupby(['dataset'])['size_right'].transform('mean')\n",
    "df['mean_skipped_left'] = df.groupby(['dataset'])['skipped_left'].transform('mean')\n",
    "df['mean_skipped_right'] = df.groupby(['dataset'])['skipped_right'].transform('mean')\n",
    "df['mean_skipped'] = df.mean_skipped_left + df.mean_skipped_right \n",
    "\n",
    "df['total_sum'] = df.mean_hits + df.mean_miss_left + df.mean_miss_right + df.mean_skipped_left + df.mean_skipped_right \n",
    "\n",
    "df['right_idx'] = 1\n",
    "df['right_vlmc_cumsum'] = df.groupby('dataset')['right_idx'].transform('cumsum')\n",
    "\n",
    "df_medium = df[(df.left_vlmc==0) & (df.right_vlmc_cumsum==1) & (df.dataset_bool==True)]\n",
    "\n",
    "df = pd.read_csv(\"tmp/integer_rep_distributions/shared-kmers/distribution_large.txt\", sep=\",\", header=None)\n",
    "df.columns = ['input']\n",
    "\n",
    "df['dataset'] = df.input.apply(lambda x : x.split('_')[0] + \" to \" + x.split('_')[2])\n",
    "df['dataset_bool'] = df.dataset.apply(lambda x : x in keep_sets)\n",
    "df['left_vlmc'] = df.input.apply(lambda x : int(x.split('_')[1]))\n",
    "df['right_vlmc'] = df.input.apply(lambda x : int(x.split('_')[3]))\n",
    "df['hits'] = df.input.apply(lambda x : int(x.split('_')[4]))\n",
    "df['miss_left'] = df.input.apply(lambda x : int(x.split('_')[5]))\n",
    "df['miss_right'] = df.input.apply(lambda x : int(x.split('_')[6]))\n",
    "df['size_left'] = df.input.apply(lambda x : int(x.split('_')[7]))\n",
    "df['size_right'] = df.input.apply(lambda x : int(x.split('_')[8]))\n",
    "df['skipped_left'] = df.input.apply(lambda x : int(x.split('_')[9]))\n",
    "df['skipped_right'] = df.input.apply(lambda x : int(x.split('_')[10]))\n",
    "\n",
    "df['mean_hits'] = df.groupby(['dataset'])['hits'].transform('mean')\n",
    "df['mean_miss_left'] = df.groupby(['dataset'])['miss_left'].transform('mean')\n",
    "df['mean_miss_right'] = df.groupby(['dataset'])['miss_right'].transform('mean')\n",
    "df['mean_misses'] = df.mean_miss_left + df.mean_miss_right \n",
    "df['mean_size_left'] = df.groupby(['dataset'])['size_left'].transform('mean')\n",
    "df['mean_size_right'] = df.groupby(['dataset'])['size_right'].transform('mean')\n",
    "df['mean_skipped_left'] = df.groupby(['dataset'])['skipped_left'].transform('mean')\n",
    "df['mean_skipped_right'] = df.groupby(['dataset'])['skipped_right'].transform('mean')\n",
    "df['mean_skipped'] = df.mean_skipped_left + df.mean_skipped_right \n",
    "\n",
    "df['total_sum'] = df.mean_hits + df.mean_miss_left + df.mean_miss_right + df.mean_skipped_left + df.mean_skipped_right \n",
    "\n",
    "df['right_idx'] = 1\n",
    "df['right_vlmc_cumsum'] = df.groupby('dataset')['right_idx'].transform('cumsum')\n",
    "\n",
    "df_large = df[(df.left_vlmc==0) & (df.right_vlmc_cumsum==1) & (df.dataset_bool==True)]\n",
    "\n",
    "colors = px.colors.qualitative.Plotly\n",
    "hit_color = colors[0]\n",
    "miss_color = colors[4]\n",
    "skip_color = colors[2]\n",
    "\n",
    "tfs = 19\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, shared_yaxes=False, shared_xaxes=True,\n",
    "                        y_title=\"Percentage of k-mers\",\n",
    "                        subplot_titles=(\"Small\", \"Medium\", \"Large\"), \n",
    "                        horizontal_spacing= 0.02, vertical_spacing= 0.1)\n",
    "\n",
    "df_small['dataset'] = df_small.dataset.apply(lambda x : x.replace('ecoli', 'virus'))\n",
    "df_medium['dataset'] = df_medium.dataset.apply(lambda x : x.replace('ecoli', 'virus'))\n",
    "df_large['dataset'] = df_large.dataset.apply(lambda x : x.replace('ecoli', 'virus'))\n",
    "\n",
    "convert_dict = {'human to human':0, 'human to turkey':1, 'human to corn':2, 'turkey to turkey':3, 'turkey to corn':4, 'corn to corn':5, 'virus to virus':6}\n",
    "\n",
    "df_small['new'] = df_small['dataset'].map(convert_dict)\n",
    "df_medium['new'] = df_medium['dataset'].map(convert_dict)\n",
    "df_large['new'] = df_large['dataset'].map(convert_dict)\n",
    "\n",
    "df_small.sort_values(by=['new'], ignore_index=True, inplace=True)\n",
    "df_small.drop(columns=['new'], inplace=True)\n",
    "\n",
    "df_medium.sort_values(by=['new'], ignore_index=True, inplace=True)\n",
    "df_medium.drop(columns=['new'], inplace=True)\n",
    "\n",
    "df_large.sort_values(by=['new'], ignore_index=True, inplace=True)\n",
    "df_large.drop(columns=['new'], inplace=True)\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_small.dataset, y=(df_small.mean_hits * 100) / df_small.total_sum, marker_color=hit_color, name='hits'), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=df_small.dataset, y=(df_small.mean_misses * 100) / df_small.total_sum, marker_color=miss_color, name='misses'), row=1, col=1) # , text=df_small.mean_misses.apply(round), textfont_size=tfs), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=df_small.dataset, y=(df_small.mean_skipped * 100) / df_small.total_sum, marker_color=skip_color, name='skipped'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_medium.dataset, y=(df_medium.mean_hits * 100) / df_medium.total_sum, marker_color=hit_color, name='hits', showlegend=False), row=2, col=1)\n",
    "fig.add_trace(go.Bar(x=df_medium.dataset, y=(df_medium.mean_misses * 100) / df_medium.total_sum, marker_color=miss_color, name='misses', showlegend=False), row=2, col=1) # , text=df_medium.mean_misses.apply(round), textfont_size=tfs), row=2, col=1)\n",
    "fig.add_trace(go.Bar(x=df_medium.dataset, y=(df_medium.mean_skipped * 100) / df_medium.total_sum, marker_color=skip_color, name='skipped', showlegend=False), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_large.dataset, y=(df_large.mean_hits * 100) / df_large.total_sum, marker_color=hit_color, name='hits', showlegend=False), row=3, col=1)\n",
    "fig.add_trace(go.Bar(x=df_large.dataset, y=(df_large.mean_misses * 100) / df_large.total_sum, marker_color=miss_color, name='misses', showlegend=False), row=3, col=1) # , text=df_large.mean_misses.apply(round), textfont_size=tfs), row=3, col=1)\n",
    "fig.add_trace(go.Bar(x=df_large.dataset, y=(df_large.mean_skipped * 100) / df_large.total_sum, marker_color=skip_color, name='skipped', showlegend=False), row=3, col=1)\n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        font_size=24,\n",
    "        y=1.08),\n",
    "        margin=dict(l=100, r=60, t=60, b=80),\n",
    "        height=900,\n",
    "        width=1200,\n",
    "        plot_bgcolor=\"white\",\n",
    "        barmode='stack')\n",
    "\n",
    "for i in range(0, len(fig.layout.annotations)):\n",
    "        fig.layout.annotations[i][\"font\"] = {'size': 24}\n",
    "\n",
    "fig.update_xaxes(tickfont_size=20, showline=True, linewidth=1, linecolor='LightGrey')\n",
    "fig.update_yaxes(tickfont_size=20, showline=True, linewidth=1, linecolor='LightGrey')\n",
    "fig.update_traces(textposition='inside')\n",
    "fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
    "fig.write_image(\"images/missed-percentage.pdf\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating GC-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"data/corn_fasta_files\"\n",
    "\n",
    "count = 0\n",
    "sum_gc_percent = 0\n",
    "sum_n_percent = 0\n",
    "for file in os.listdir(directory):\n",
    "    tot_AT = 0\n",
    "    tot_GC = 0\n",
    "    tot_N = 0\n",
    "    with open(directory + \"/\" + file) as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(0, len(lines)):\n",
    "            s = lines[i].strip()\n",
    "            if (i != 0):\n",
    "                if not s.__contains__(\">\"):\n",
    "                    if s.__contains__(\"<\"):\n",
    "                        print(s)\n",
    "                    tot_GC += s.count('C')\n",
    "                    tot_GC += s.count('c')\n",
    "                    tot_GC += s.count('G')\n",
    "                    tot_GC += s.count('g')\n",
    "                    tot_AT += s.count('A')\n",
    "                    tot_AT += s.count('a')\n",
    "                    tot_AT += s.count('T')\n",
    "                    tot_AT += s.count('t')\n",
    "                    tot_N += s.count('N')\n",
    "                    tot_N += s.count('n')\n",
    "    if (tot_AT != 0):\n",
    "        sum_gc_percent += (tot_GC * 100) / (tot_AT + tot_GC)\n",
    "        sum_n_percent += (tot_N * 100) / (tot_AT + tot_GC + tot_N)\n",
    "        count += 1 \n",
    "\n",
    "print(sum_gc_percent / count)\n",
    "\n",
    "print(sum_n_percent / count)\n",
    "\n",
    "# Ecoli 45.79413717686636 - N 0.013576374637253287\n",
    "# Human 41.21919313532962 - N 0.0\n",
    "# Turkey 44.343682123631694 - N 3.148234906651108\n",
    "# Corn 46.82643413252128 - N 0.17459701201471825"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misses in a row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"tmp/integer_rep_distributions/missed-in-a-row/new-actual-counts/\"\n",
    "\n",
    "read_datasets = {\n",
    "    \"dataset\" : [],\n",
    "    \"count\" : []\n",
    "}\n",
    "read_datasets = pd.DataFrame(read_datasets)\n",
    "alldirs = []\n",
    "for file in os.listdir(directory):\n",
    "    ds = file.split('_')[0] + \"_\" + file.split('_')[2]\n",
    "    if not (ds in list(read_datasets['dataset'])):\n",
    "        read_datasets.loc[-1] = [ds, 0]\n",
    "        read_datasets.index = read_datasets.index + 1  # shifting index\n",
    "        read_datasets = read_datasets.sort_index()\n",
    "        df_tmp = pd.read_csv(directory + file, sep=\",\", header=None)\n",
    "        df_tmp.columns = ['input']\n",
    "        df_tmp['dataset'] = file.split('_')[0] + \" to \" + file.split('_')[2]\n",
    "        df_tmp['vlmc_left'] = file.split('_')[1]\n",
    "        df_tmp['vlmc_right'] = file.split('_')[3].split('.')[0]\n",
    "        df_tmp['left_idx'] = df_tmp['input'].apply(lambda x : int(x.split('_')[0]))\n",
    "        df_tmp['right_idx'] = df_tmp['input'].apply(lambda x : int(x.split('_')[1]))\n",
    "        df_tmp['misses_in_a_row'] = df_tmp['input'].apply(lambda x : int(x.split('_')[2]))\n",
    "        alldirs.append(df_tmp)\n",
    "    else:\n",
    "        i = read_datasets[read_datasets.dataset==ds]['count'].iloc[0]\n",
    "        if (i < 3):\n",
    "            read_datasets.loc[read_datasets.dataset==ds, 'count'] = i + 1\n",
    "            df_tmp = pd.read_csv(directory + file, sep=\",\", header=None)\n",
    "            df_tmp.columns = ['input']\n",
    "            df_tmp['dataset'] = file.split('_')[0] + \" to \" + file.split('_')[2]\n",
    "            df_tmp['vlmc_left'] = file.split('_')[1]\n",
    "            df_tmp['vlmc_right'] = file.split('_')[3].split('.')[0]\n",
    "            df_tmp['left_idx'] = df_tmp['input'].apply(lambda x : int(x.split('_')[0]))\n",
    "            df_tmp['right_idx'] = df_tmp['input'].apply(lambda x : int(x.split('_')[1]))\n",
    "            df_tmp['misses_in_a_row'] = df_tmp['input'].apply(lambda x : int(x.split('_')[2]))\n",
    "            alldirs.append(df_tmp)\n",
    "\n",
    "df = pd.concat(alldirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_x = 2000\n",
    "iloc_x = 200\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for ds in df.dataset.unique():\n",
    "    df_ds = df[df.dataset==ds]\n",
    "    df_ds['rolling_misses'] = df_tmp.misses_in_a_row.rolling(rolling_x).mean()\n",
    "    df_ds['mean_missess'] = df_ds.groupby('left_idx')['rolling_misses'].transform('mean')\n",
    "    df_ds = df_ds[(df_ds.vlmc_left==df_ds.vlmc_left.unique()[0]) & (df_ds.vlmc_right==df_ds.vlmc_right.unique()[0])]\n",
    "    df_ds = df_ds.iloc[::iloc_x]\n",
    "    fig.add_trace(go.Scatter(x=df_ds['right_idx'], y=df_ds['rolling_misses'], name=ds))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"tmp/integer_rep_distributions/missed-in-a-row/new-actual-counts/\"\n",
    "\n",
    "alldirs = []\n",
    "for file in os.listdir(directory):\n",
    "    try:\n",
    "        df_tmp = pd.read_csv(directory + file, sep=\",\", header=None)\n",
    "        df_tmp.columns = ['input']\n",
    "        df_tmp['dataset'] = file.split('_')[0] + \" to \" + file.split('_')[2]\n",
    "        df_tmp['vlmc_left'] = file.split('_')[1]\n",
    "        df_tmp['vlmc_right'] = file.split('_')[3].split('.')[0]\n",
    "        df_tmp['slice'] = df_tmp['input'].apply(lambda x : int(x.split('_')[0]))\n",
    "        df_tmp['sum'] = df_tmp['input'].apply(lambda x : int(x.split('_')[1]))\n",
    "        df_tmp['count'] = df_tmp['input'].apply(lambda x : int(x.split('_')[2]))\n",
    "        df_tmp['avg'] = df_tmp['input'].apply(lambda x : int(x.split('_')[3]))\n",
    "        alldirs.append(df_tmp)\n",
    "    except: \n",
    "        print(file)\n",
    "\n",
    "df = pd.concat(alldirs)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    if (x > 0):\n",
    "        return math.log(x, 10)\n",
    "    else:\n",
    "        return x \n",
    "\n",
    "df['new_avg'] = df['sum'] / df['count'] \n",
    "df['avg_avg'] = df.groupby(['dataset', 'slice'])['new_avg'].transform('mean')\n",
    "\n",
    "df_tmp = df[(df.vlmc_left=='0') & (df.vlmc_right=='0')]\n",
    "df_tmp['avg_avg'] = df_tmp.avg_avg.apply(fun)\n",
    "\n",
    "fig = px.line(df_tmp, x='slice', y='avg_avg', color='dataset')\n",
    "fig = make_subplots(rows=1, \n",
    "                    cols=1, # , shared_yaxes=False, shared_xaxes=True,\n",
    "                    x_title=\"Slice of VLMCs\",\n",
    "                    y_title=\"Average misses in a row\") # specs=[[{}, {}],[{\"colspan\": 2}, None]])\n",
    "\n",
    "line_dashes = ['solid', 'dot', 'dash', 'longdash', 'dashdot', 'longdashdot', 'solid']\n",
    "implementations = [\"human to human\", \"human to turkey\", \"human to corn\", \"turkey to turkey\", \"turkey to corn\", \"corn to corn\", \"ecoli to ecoli\"]\n",
    "line_color_map = list(zip(implementations, px.colors.qualitative.Plotly))\n",
    "iterate_on = list(zip(list(implementations), line_dashes, ['#00CC96', px.colors.qualitative.Plotly[5], px.colors.qualitative.Plotly[7], '#FECB52', px.colors.qualitative.Plotly[4], '#EF553B', '#636EFA']))\n",
    "print(iterate_on)\n",
    "for imp, line_dash, color in iterate_on:\n",
    "    df_imp = df_tmp[df_tmp.dataset==imp]\n",
    "    fig.add_trace(go.Scatter(x=df_imp['slice'], y=df_imp['avg_avg'], name=imp, mode='lines', line_color=color, line_dash=line_dash), 1, 1) # line_color=line_color, line_dash=line_dash,\n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"h\",\n",
    "    yanchor=\"bottom\",\n",
    "    font_size=24,\n",
    "    y=1.08),\n",
    "    margin=dict(l=100, r=60, t=60, b=80),\n",
    "    height=600,\n",
    "    width=900,\n",
    "    plot_bgcolor=\"white\")\n",
    "\n",
    "fig.update_xaxes(gridcolor='LightGrey', tickfont_size=20, showline=True, linewidth=1, linecolor='LightGrey')\n",
    "fig.update_yaxes(# tickmode=\"array\",\n",
    "                #  tickvals=[-15, -12, -9, -6, -3, 0, 3, 6, 9, 12, 15],\n",
    "                 # ticktext=[\"2^-15\", \"2^-12\", \"2^-9\", \"2^-6\", \"2^-3\", \"2^0\", \"2^3\", \"2^6\", \"2^9\", \"2^12\", \"2^15\"], \n",
    "                gridcolor='LightGrey', tickfont_size=20, showline=True, linewidth=1, linecolor='LightGrey')\n",
    "\n",
    "for i in range(0, len(fig.layout.annotations)):\n",
    "    fig.layout.annotations[i][\"font\"] = {'size': 24}\n",
    "\n",
    "fig.write_image(\"images/log_average_misses.pdf\")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
